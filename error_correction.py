"""
Example script to correct prompt results generated by LLM, the functions of generation prompts are in correction_utils.py.
"""
import argparse
import os
import json
from typing import List
from pathlib import Path
import time

from tqdm import tqdm
import openai

from dataset_classes.spider_dataset import SpiderDataset
from dataset_classes.bird_dataset import BirdDataset
# from utils.template_utils import get_template, fill_template
from utils.openai_utils import init_gpt, init_openai_client, get_prompt_from_openai, num_tokens_from_messages
from utils.sql_str_utils import query_postprocessing
from utils.dataset_utils import load_line_by_line_json, save_line_by_line_json, load_txt_records
from utils.correction_utils import creating_schema, construct_self_correction_prompt, fill_error_correction_prompt
from utils.template_utils import count_tokens

def main(args):
    #### Step 1: load the dataset
    ## reason: need database id and table.json for self-correction prompt generation
    dataset_name = args.dataset_name
    dataset_dir_path = args.dataset_dir_path
    ## if the dataset_dir_path is not provided, use the default path datasets/{dataset_name}
    if not dataset_dir_path:
        dataset_dir_path = os.path.join(os.path.dirname(__file__), 'datasets', dataset_name)
    if args.dataset_name == 'spider':
        dataset = SpiderDataset(dataset_dir_path, dev_preliminary_queries_file_path=None, test_preliminary_queries_file_path=None)
    elif args.dataset_name == 'bird':
        dataset = BirdDataset(dataset_dir_path, dev_preliminary_queries_file_path=None)
    else:
        raise ValueError(f"Invalid dataset name {args.dataset_name}")
    dataset_name = args.dataset_name
    
    if args.split_name == 'dev':
        split_name = 'dev'
        table_schema_path = dataset.dev_table_schema_path
    elif args.split_name == 'test':
        table_schema_path = dataset.test_table_schema_path
        split_name = 'test'
    else:
        raise ValueError(f"Invalid split name {args.split_name}")
    
    if dataset_name == 'spider':
        db_id_list = [x['db_id'] for x in dataset.data[split_name]]
        questions = [x['question'] for x in dataset.data[split_name]]
        if args.first_n > 0:
            db_id_list = db_id_list[:args.first_n]
            questions = questions[:args.first_n]
        num_questions = len(questions)
        print(f"Loaded {len(db_id_list)} db_ids and {len(questions)} questions from {dataset_dir_path}")
    elif dataset_name == 'bird':
        db_id_list = [x['db_id'] for x in dataset.data[split_name]]
        questions = [x['question'] for x in dataset.data[split_name]]
        evidences = [x['evidence'] for x in dataset.data[split_name]]
        if args.first_n > 0:
            db_id_list = db_id_list[:args.first_n]
            questions = questions[:args.first_n]
            evidences = evidences[:args.first_n]
        num_questions = len(questions)
        print(f"Loaded {len(db_id_list)} db_ids and {len(questions)} questions from {dataset_dir_path}")

    #### Step 2: load the generated prompt results
    records = load_txt_records(args.prev_response_file_path)
    # assert len(records) == len(db_id_list), f"Number of previous prompt results {len(records)} does not match number of db_ids {len(db_id_list)}"
    print(f"Loaded {len(records)} previous prompt results from {args.prev_response_file_path}")

    ## Only run error correction on valid indices
    if args.valid_indices_file_path:
        valid_indices = load_txt_records(args.valid_indices_file_path)
        db_id_list = [db_id_list[i] for i in valid_indices]
        questions = [questions[i] for i in valid_indices]
        records = [records[i] for i in valid_indices]
        if dataset_name == 'bird':
            evidences = [evidences[i] for i in valid_indices]
        num_questions = len(questions)
        print(f"Loaded {num_questions} valid indices from {args.valid_indices_file_path}")

    #### Step 3: generate zero-shot template for self-correction
    self_correction_prompts = []
    if dataset_name == 'spider':
        spider_schema, spider_primary, spider_foreign = creating_schema(table_schema_path)
        for prev_response, db_id, question in zip(records, db_id_list, questions):
            self_correction_prompts.append(
                construct_self_correction_prompt(
                    question, db_id, prev_response, spider_schema, spider_primary, spider_foreign, rules_groups=args.rules_groups
                    )
                )
    elif dataset_name == 'bird':
        for prev_response, db_id, question, evidence in zip(records, db_id_list, questions, evidences):
            schema_text = dataset.schema_text_dict[split_name][db_id]
            prompt_text = fill_error_correction_prompt(
                question, 
                prev_response, 
                schema_text=schema_text,
                evidence=evidence,
                rules_groups=args.rules_groups
            )
            self_correction_prompts.append(prompt_text)
    else:
        raise ValueError(f"Invalid dataset name {dataset_name}")
    ## check prompt length
    for i, prompt in enumerate(self_correction_prompts):
        if len(prompt) == 0:
            print(f"Empty prompt for question {i}")
        if count_tokens(prompt, model=args.model) > 4096:
            print(f"Prompt too long for question {i}")
    print(f"Generated {len(self_correction_prompts)} self-correction prompts")
    ## save self-correction prompts to file, currently only prompt is saved
    if args.prompt_file_path:
        Path.mkdir(Path(args.prompt_file_path).parent, parents=True, exist_ok=True)
        save_line_by_line_json(self_correction_prompts, args.prompt_file_path)


    #### Step 4: generate response for each prompt
    ## init openai api
    init_gpt(args.openai_api_key, args.openai_organization)

    token_cnt = 0
    output_path = args.response_file_path
    if not output_path:
        output_file_name = os.path.splitext(os.path.basename(args.prev_response_file_path))[0] + '.txt'
        output_path = os.path.join(os.path.dirname(os.path.dirname(args.prev_response_file_path)), 'responses', output_file_name)

    Path.mkdir(Path(output_path).parent, parents=True, exist_ok=True)

    start_time = time.time()
    print(f"Generating responses for {num_questions} prompts")
    ## initialize openai client
    client = init_openai_client(args.openai_api_key, args.openai_organization)
    with open(output_path, 'w') as f:
        for i, question in enumerate(tqdm(self_correction_prompts)):
            res = get_prompt_from_openai(client, args.model, question, args.temperature, args.n)
            if res is None:
                print(f"Failed to generate response for question {i}")
                f.write("\n")
                continue
            # parse result
            token_cnt += res["total_tokens"]
            if args.n == 1:
                for sql in res["response"]:
                    sql = query_postprocessing(sql)
                    f.write(sql)
            else:
                pass
    end_time = time.time()
    print(f"Total time: {end_time - start_time}, time per question: {(end_time - start_time) / len(questions)}")
    print(f"Total tokens in results: {token_cnt}")
    print(f"Average tokens per question: {token_cnt / len(questions)}")
    print(f"Results saved to {output_path}")
    return

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Correct Errors in Generated Prompts')
    parser.add_argument('--prev_response_file_path', type=str, help='file path to store the previous prompt results', default='')
    parser.add_argument('--prompt_file_path', type=str, help='file path to store the prompts generated by error correction module', default='')
    parser.add_argument('--response_file_path', type=str, help='file path to store the prompt results', default='')
    parser.add_argument('--valid_indices_file_path', type=str, help='file path to store the valid indices that need correction', default='')
    parser.add_argument('--dataset_name', type=str, help='the name of dataset', default='spider', choices=['spider', 'bird'])
    parser.add_argument('--dataset_dir_path', type=str, help='the directory of dataset', default='')
    parser.add_argument('--split_name', type=str, help='the name of split', default='dev')
    parser.add_argument("--openai_api_key", type=str)
    parser.add_argument("--openai_organization", type=str, default="")
    parser.add_argument("--model", type=str, default="gpt-4")
    parser.add_argument("--temperature", type=float, default=0)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument("--n", type=int, default=1, help="# of sqls to generate for each question")
    parser.add_argument("--first_n", type=int, help="only run first n records", default=100)

    ## rules for self-correction
    parser.add_argument("--rules_groups", type=int, nargs='+', help="rules groups to use for self-correction", default=[1])
    
    main(parser.parse_args())