import os
import json

from nltk import word_tokenize

from dataset_classes.base_dataset import BaseDataset
# from .base_dataset import BaseDataset
from utils.dataset_utils import load_json_records, load_txt_records
from utils.hardness_utils import eval_hardness

class SpiderDataset(BaseDataset):
    dataset_name = "spider"

    train_file_name = "train_spider_and_others.json"
    train_sql_file_name = "train_gold.sql"
    train_database_dir_name = "database"
    train_table_schema_file_name = "tables.json"

    flag_has_dev = True
    dev_file_name = "dev.json"
    dev_sql_file_name = "dev_gold.sql"
    dev_database_dir_name = "database"
    dev_table_schema_file_name = "tables.json"

    flag_has_test = True
    test_file_name = "test.json"
    test_sql_file_name = "test_gold.sql"
    test_database_dir_name = "test_database"
    test_table_schema_file_name = "test_tables.json"

    def __init__(self, dataset_dir_path, dev_preliminary_queries_file_path=None, test_preliminary_queries_file_path=None):
        super().__init__(dataset_dir_path)
        if dev_preliminary_queries_file_path is not None:
            self.load_preliminary_queries('dev', dev_preliminary_queries_file_path)
        if test_preliminary_queries_file_path is not None:
            self.load_preliminary_queries('test', test_preliminary_queries_file_path)

    def assign_idx_to_record(self):
        """Add one additional key "id" with index of record in the split
        """
        for split_name in self.data.keys():
            for i, record in enumerate(self.data[split_name]):
                record['idx'] = i
        return
    
    def add_database_path(self):
        """Store the database path in each record
        """
        self.db_paths = dict()
        for split_name in self.data.keys():
            self.db_paths[split_name] = dict()
            for record in self.data[split_name]:
                ## database dir path according to the split train dev and test
                database_dir_path = ""
                if split_name == 'train':
                    database_dir_path = self.train_database_dir_path
                elif split_name == 'dev':
                    database_dir_path = self.dev_database_dir_path
                elif split_name == 'test':
                    database_dir_path = self.test_database_dir_path
                else:
                    raise ValueError(f"Invalid split name {split_name}")
                db_path = os.path.join(database_dir_path, record['db_id'], record['db_id']+'.sqlite')
                record['db_path'] = db_path
                self.db_paths[split_name][record['db_id']] = db_path
        return

    def tokenize_questions(self):
        """Tokenize the questions. Tokenization results are already stored in the dataset file.
        """
        pass

    def tokenize_queries(self):
        """Tokenize the SQL queries. Tokenization results are already stored in the dataset file.
        """
        pass

    def load_splits(self):
        """Load the train and test splits
        """
        # self.train_data = load_json_records(self.train_file_path)
        split_names = ['train', 'dev', 'test']
        for split_name, file_path, file_sql_path in zip(split_names, [self.train_file_path, self.dev_file_path, self.test_file_path], [self.train_sql_file_path, self.dev_sql_file_path, self.test_sql_file_path]):
            if file_path is None:
                # skip the split if the file path is None
                continue
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File {file_path} not found.")
            self.data[split_name] = load_json_records(file_path)
            print(f"Loaded {len(self.data[split_name])} records for {split_name} split.")
        return
    
    def load_preliminary_queries(self, split_name, preliminary_queries_file_path):
        """Load the preliminary queries generated by text2sql to overwrite the gold queries (query, query_toks) in the Spider dataset.
        """
        if not os.path.exists(preliminary_queries_file_path):
            # use a null query instead
            for record in self.data[split_name]:
                record['query'] = ""
                record['query_toks'] = []
        else:
            preliminary_queries = load_txt_records(preliminary_queries_file_path)
            for record, sql in zip(self.data[split_name], preliminary_queries):
                record['query'] = sql
                ## option 1: simple tokenization by nltk
                record['query_toks'] = [word.lower() for word in word_tokenize(sql)]
                ## option 2: use the tokenize function in Spider repo (https://github.com/taoyds/spider/blob/master/process_sql.py)
                # record['query_toks'] = tokenize(sql)
        return
    
    def load_data(self):
        """Load the whole dataset
        """
        self.load_splits()
        self.assign_idx_to_record()
        self.tokenize_questions()
        self.tokenize_queries()
        self.add_database_path()
        print("Dataset loaded.")
        return
    
    def load_schema(self):
        """Load the table schema for the dataset
        """
        self.table_schema = dict()
        self.table_schema['train'] = self.load_schema_file(self.train_table_schema_path)
        if self.flag_has_dev:
            self.table_schema['dev'] = self.load_schema_file(self.dev_table_schema_path)
        if self.flag_has_test:
            self.table_schema['test'] = self.load_schema_file(self.test_table_schema_path)
        return
    
    def load_schema_file(self, schema_file_path):
        """Load the table schema for the dataset
        """
        with open(schema_file_path, 'r') as f:
            table_schema = json.load(f)
        ## change list of tables to dict of tables
        table_schema = {table['db_id']: table for table in table_schema}
        print(f"Loaded table schema for {len(table_schema)} tables.")
        return table_schema
